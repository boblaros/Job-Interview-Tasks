# Kaspersky Lab NLP Test

This folder contains my solution for an NLP assessment that was part of the hiring process for an NLP Engineer position at Kaspersky Lab. The task was to evaluate the quality of a proprietary library that extracts the main text of news articles.

## Project Structure

- `solution-kutivadze.ipynb` – the main Jupyter notebook with the full analysis and code.
- `solution-kutivadze.pdf` – a static export of the notebook for quick review.
- `Test_check.xlsx` – the source spreadsheet supplied with URLs and the library output for each news item.
- `output-data.csv` – the enriched dataset generated by the notebook with all computed metrics.

## Problem Statement

For every news article the library provides its extracted text. The goal was to:

1. Parse each article to obtain a reference (ground-truth) version of the full text.
2. Measure how complete the library output is in comparison with the reference text.
3. Estimate how important the missing fragments are.
4. Provide recommendations on how to improve the extraction pipeline.

The expected deliverable was a table containing the reference text, completeness scores, significance of the missing content, and any improvement notes.

## Methodology

The solution is implemented in the notebook and follows these steps:

1. **Data ingestion** – load the spreadsheet, inspect the columns, and verify the absence of missing values.
2. **Full text retrieval** – download each article by URL with [`newspaper3k`](https://github.com/codelucas/newspaper) and add the parsed text as a new column.
3. **Completeness scoring** – encode the library text and the full article using the `paraphrase-MiniLM-L6-v2` SentenceTransformer model. Their cosine similarity serves as a semantic completeness score (values close to 1 mean high coverage).
4. **Uncovered text extraction** – align the two texts with `difflib.SequenceMatcher` to capture the fragments of the reference article that were not returned by the library.
5. **Significance estimation** – assess the importance of the missing fragments with two complementary metrics:
   - **TF–IDF importance** – compute TF–IDF scores (via `scikit-learn`) for all missing fragments and sum the top keywords for each article.
   - **Semantic gap** – measure the cosine distance between TF–IDF representations of the extracted and full texts to quantify how semantically different they are.
6. **Visualization & interpretation** – plot histograms for the semantic gap and TF–IDF significance to identify problematic articles and derive recommendations (e.g., set alert thresholds for high semantic gaps).

All intermediate results and plots can be found in the notebook.

## How to Reproduce

1. Create a Python 3.9+ environment.
2. Install the required packages:

   ```bash
   pip install pandas newspaper3k sentence-transformers matplotlib nltk scikit-learn
   ```

3. Run the notebook `solution-kutivadze.ipynb` from start to finish. The notebook will:
   - Download any missing NLTK stopwords (Russian).
   - Parse the news articles.
   - Compute the metrics and export the enriched table to `output-data.csv`.

> **Note:** Parsing articles requires internet access because the notebook fetches each URL in `Test_check.xlsx`.

## Results

- **Completeness scores** are generally high, but a subset of articles exhibits noticeable gaps.
- **Semantic gap** histograms highlight a small group of news items with significant divergence between the library output and the reference text.
- **TF–IDF significance** shows the distribution of keyword importance in the missing fragments, which helps monitor trends over time.

## Recommendations

- Introduce an automated alert when the semantic gap exceeds a threshold (e.g., 0.10–0.15) to flag articles for manual review.
- Investigate the high-significance omissions to understand recurring failure modes and improve the extraction heuristics.

For detailed code, outputs, and commentary, please open the Jupyter notebook or the accompanying PDF export.
